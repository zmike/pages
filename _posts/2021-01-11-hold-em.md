---
published: false
---
## A Different Strategy

As the merge window for the upcoming Mesa release looms, Erik and I have decided on a new strategy for development: we're just going to stop merging patches.

At this point in time, we have no regressions as compared to the last release, so we're just doing a full stop until after the branch point in order to save ourselves time potentially tracking down any issues in further feature additions.

## Slowdowns
Some of you may have noticed that zink-wip has yet to update this year. This isn't due to a lack of work, but rather due to lack of stability. I've been tinkering with a new descriptor management infrastructure (yes, I'm back on the horse), and it's... _capable of drawing frames_ is maybe the best way to describe it. I've gone through probably about ten iterations on it so far based on all the ideas I've had.

This is hardly an exhaustive list, but here's some of the ideas that I've cycled through:
* **async descriptor updating** - It seems like this should be good on paper given that it's legal to do descriptor updates in threads, but the overhead from signalling the task thread in this case ended up being, on average, about 10-20x the cost of just doing the updating synchronously.

* **all push descriptors all the time** - Just for hahas along the way I jammed everything into a [pushed descriptor set](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VK_KHR_push_descriptor.html). Or at least I was going to try. About halfway through, I realized this was way more work to execute than it'd be worth for the hahas considering I wouldn't ever be able to use this in reality.

* **zero iteration updates** - The gist of this ideas is that looking at the descriptor updating code, there's a ton of iterating going on. This is an extreme hotpath, so any amount of looping that can be avoided is great, and the underlying Vulkan driver has to iterate the sets anyway, so... Eventually I managed to throw a bunch of memory at the problem and do all the setup during pipeline init, giving me pre-initialized blobs of memory in the form of [VkWriteDescriptorSet](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkWriteDescriptorSet.html) arrays with the associated sub-types for descriptors. With this in place, naturally I turned to...

* **templates** - [Descriptor templates](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VK_KHR_descriptor_update_template.html) are a way of giving the Vulkan driver the raw memory of the descriptor info as a blob and letting it huck that directly into a buffer. Since I already had the memory set up for this, it was an easy swap over, though the gains were less impressive than I'd expected.

At last I've settled on a model of uncached, templated descriptors with an extra push set for handling uniform data for further exploration. Initial results for real world use (e.g., graphical benchmarks) are good, but piglit's `drawoverhead` test shows there's still a lot of work to be done to catch up to caching.

Big thanks to Hans-Kristian Arntzen, aka [themaister](https://themaister.net/blog/), aka low-level graphics swashbuckler, for providing insight and consults along the process of this.